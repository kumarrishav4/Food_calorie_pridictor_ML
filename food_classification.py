# -*- coding: utf-8 -*-
"""Food-Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XWHqASJXFD0v0o2RSgV_5fHjkHj7l93c
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import os.path
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

"""# Image Directory"""

image_dir = Path('/Users/datascientist/Downloads/Food detction/Indian Food Images')

"""# Setting Filepath and Labels"""

# Check if the directory exists
if not image_dir.exists():
    raise FileNotFoundError(f"The directory '{image_dir}' does not exist.")

# Get all jpg file paths
filepath = list(image_dir.glob('**/*.jpg'))

# Verify that file paths are found
if not filepath:
    raise ValueError(f"No image files found in the directory '{image_dir}'.")

# Extract labels using a list comprehension
label = [os.path.split(os.path.split(path)[0])[1] for path in filepath]

"""# Converting it into image_df"""

# Create a DataFrame
image_df = pd.DataFrame({
    'Filepath': filepath,
    'Label': label
})

# Verify DataFrame content
if image_df.empty:
    raise ValueError("The DataFrame is empty. Check the image directory and labels.")

# Shuffle the DataFrame
image_df = image_df.sample(frac=1.0, random_state=1).reset_index(drop=True)

print(f"Number of unique labels: {image_df['Label'].nunique()}")

print(f"Shape of DataFrame: {image_df.shape}")

"""# Label encoding Food names"""

# Encode the labels
le = LabelEncoder()
image_df['Label'] = le.fit_transform(image_df['Label'])

# Display label distribution
print(image_df['Label'].value_counts())

"""# Image Data Generator

#### Creating train_test_split
"""

train_df, test_df = train_test_split(image_df, test_size=0.3, shuffle=True, random_state=42)

train_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,validation_split = 0.2)
test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)

# Convert numeric labels to strings if needed
train_df['Label'] = train_df['Label'].astype(str)
test_df['Label'] = test_df['Label'].astype(str)

# Convert Filepath column to strings
train_df['Filepath'] = train_df['Filepath'].astype(str)
test_df['Filepath'] = test_df['Filepath'].astype(str)

# Ensure file paths are valid
train_df = train_df[train_df['Filepath'].apply(os.path.exists)]
test_df = test_df[test_df['Filepath'].apply(os.path.exists)]

# Check if the DataFrame is empty
if train_df.empty or test_df.empty:
    raise ValueError("No valid file paths found in the dataset. Please check the image directory.")

# Define the common parameters for the ImageDataGenerator
common_params = {
    'dataframe': train_df,
    'x_col': 'Filepath',
    'y_col': 'Label',
    'target_size': (224, 224),
    'batch_size': 32,
    'color_mode': 'rgb',
    'class_mode': 'categorical',
    'shuffle': True,
    'seed': 42
}

# Create train and validation generators from the same training dataframe
train_image = train_gen.flow_from_dataframe(**common_params, subset='training')
val_image = train_gen.flow_from_dataframe(**common_params, subset='validation')

# Create test generator from the test dataframe
test_image = test_gen.flow_from_dataframe(
    dataframe=test_df,
    x_col='Filepath',
    y_col='Label',
    target_size=(224, 224),
    batch_size=32,
    color_mode='rgb',
    class_mode='categorical',
    shuffle=False
)

"""# Training"""

# Define the model architecture
def create_model(input_shape=(224, 224, 3), num_classes=1):
    inputs = tf.keras.Input(shape=input_shape)

    # Convolutional and pooling layers
    x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)
    x = tf.keras.layers.MaxPool2D()(x)
    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)
    x = tf.keras.layers.MaxPool2D()(x)

    # Global pooling layer
    x = tf.keras.layers.GlobalAveragePooling2D()(x)

    # Dense layers
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)

    # Output layer
    outputs = tf.keras.layers.Dense(num_classes, activation='linear')(x)

    # Create the model
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    return model

# Create an instance of the model
model = create_model(input_shape=(224, 224, 3), num_classes=1)

# Compile the model
model.compile(optimizer='adam',
              loss='mse',
              metrics=['accuracy'])

# Define early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Train the model
history = model.fit(
    train_image,
    validation_data=val_image,
    epochs=5,
    callbacks=[early_stopping])

"""Results"""

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_image, verbose=0)

# Print the results with formatted strings
print(f'Test Loss: {test_loss:.3f}')
print(f'Test Accuracy: {test_accuracy * 100:.3f}%')

"""# Plotting Training and Validation (Loss & Accuracy)"""

# Define the number of epochs for plotting
epochs = range(len(history.history['accuracy']))

# Create a figure with appropriate size
plt.figure(figsize=(15, 6))

# Plot Training and Validation Accuracy
plt.subplot(1, 2, 1)  # 1 row, 2 columns, plot 1
plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')
plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Plot Training and Validation Loss
plt.subplot(1, 2, 2)  # 1 row, 2 columns, plot 2
plt.plot(epochs, history.history['loss'], label='Training Loss')
plt.plot(epochs, history.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='upper right')

# Display the plots
plt.tight_layout()  # Adjust subplots to fit into the figure area
plt.show()

"""#MLP MODEL"""

# Updated MLP Model
mlp_inputs = tf.keras.Input(shape=(224, 224, 3))
x = tf.keras.layers.Flatten()(mlp_inputs)  # Apply Flatten layer
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
mlp_outputs = tf.keras.layers.Dense(len(le.classes_), activation='softmax')(x)  # Output layer with softmax

mlp_model = tf.keras.Model(inputs=mlp_inputs, outputs=mlp_outputs)
mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

mlp_model.summary()

from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(image_df['Label']),
    y=image_df['Label']
)
class_weights_dict = dict(enumerate(class_weights))
print(class_weights_dict)

# Train the updated model
history_mlp = mlp_model.fit(
    train_image,
    validation_data=val_image,
    epochs=10,
    class_weight=class_weights_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

result = mlp_model.evaluate(test_image, verbose = 0)
print('Test Loss : {:.3f}'.format(result[0]))
print('Test Accuracy : {:.3f}%'.format(result[1]*100))

# Determine the actual number of epochs from the history object
epochs = range(len(history_mlp.history['accuracy']))

plt.figure(figsize=(20, 20))

# Plot Training and Validation Accuracy
plt.subplot(2, 2, 1)
plt.plot(epochs, history_mlp.history['accuracy'], label='Training Accuracy')
plt.plot(epochs, history_mlp.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training Accuracy vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Plot Training and Validation Loss
plt.subplot(2, 2, 2)
plt.plot(epochs, history_mlp.history['loss'], label='Training Loss')
plt.plot(epochs, history_mlp.history['val_loss'], label='Validation Loss')
plt.title('Training Loss vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.tight_layout()
plt.show()

import pickle

# Save the LabelEncoder to a file
with open("label_encoder.pkl", "wb") as file:
    pickle.dump(le, file)
print("LabelEncoder saved successfully.")

mlp_model.save("mlp_model.h5")
print("Model saved successfully.")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import pickle
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Load the saved food detection model
try:
    model = load_model("mlp_model.h5", compile=False)  # Avoid compilation issues during loading
    print("Model loaded successfully.")
except Exception as e:
    print(f"Error loading model: {e}")

# Load the saved LabelEncoder
try:
    with open("label_encoder.pkl", "rb") as file:
        label_encoder = pickle.load(file)
    print("LabelEncoder loaded successfully.")
except Exception as e:
    print(f"Error loading LabelEncoder: {e}")

def preprocess_image(image_path, target_size=(224, 224)):
    """
    Preprocess the input image for prediction.

    Args:
        image_path: Path to the input image.
        target_size: Target size for resizing the image.

    Returns:
        Preprocessed image ready for prediction.
    """
    try:
        # Load the image
        image = load_img(image_path, target_size=target_size)
        # Convert the image to an array
        image_array = img_to_array(image)
        # Scale the pixel values
        image_array = image_array / 255.0
        # Expand dimensions to create batch size of 1
        return np.expand_dims(image_array, axis=0)
    except Exception as e:
        print(f"Error preprocessing image: {e}")
        return None

def predict_image_class(image_path):
    try:
        preprocessed_image = preprocess_image(image_path)
        if preprocessed_image is None:
            return "Image preprocessing failed."
        # Predict the label
        predictions = model.predict(preprocessed_image)
        predicted_class = np.argmax(predictions, axis=1)[0]  # Get the class index
        decoded_label = label_encoder.inverse_transform([predicted_class])[0]
        print(f"Predicted Class Index: {predicted_class}")
        return decoded_label
    except Exception as e:
        print(f"Error predicting class: {e}")
        return "Prediction failed."


# Example usage
image_path = "/Users/datascientist/Downloads/Food detction/Indian Food Images/Indian Food Images/biryani/29ff2d4476.jpg"  # Replace with your image path
predicted_class = predict_image_class(image_path)
print(f"Predicted Class: {predicted_class}")

# Load the scaler
with open("scaler.pkl", "rb") as scaler_file:
    scaler = pickle.load(scaler_file)

import pickle

# Load the provided Random Forest Regressor calorie model from the uploaded file
model_path = 'calorie_prediction.pkl'
with open(model_path, 'rb') as file:
    calorie_model = pickle.load(file)

# Function to get predictions from the loaded calorie model
def predict_calories(total_fat, cholesterol, sodium, calcium, sugars, saturated_fat, serving_size, vitamin_c):

    # Prepare the input as a DataFrame
    input_data = pd.DataFrame([[
        total_fat, cholesterol, sodium, calcium, sugars, saturated_fat, serving_size, vitamin_c
    ]], columns=['total_fat', 'cholesterol', 'sodium', 'calcium',
                 'sugars', 'saturated_fat', 'serving_size', 'vitamin_c'])


    # Scale the features
    input_scaled = scaler.transform(input_data)

    # Predict using the model
    predicted_calories = calorie_model.predict(input_scaled)

    return predicted_calories[0]

"""Features used to train model and should be placed as arguments to get calories prediction


['name', 'total_fat', 'cholesterol', 'sodium', 'calcium',
          'sugars', 'saturated_fat', 'serving_size', 'vitamin_c']
"""

calorie_prediction = predict_calories(0.3, 0, 1, 5, 14, 0.1, 118, 8.7)
print(f"Predicted Calories: {calorie_prediction}")

